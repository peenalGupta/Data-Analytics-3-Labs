{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peenalGupta/Data-Analytics-3-Labs/blob/main/14_Prediction_Miniapp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZaPRLE8E1g"
      },
      "source": [
        "# Building a prediction \"miniapp\"\n",
        "\n",
        "Assuming we have already successfully trained a model for image classification (or in our case someone else did on [ImageNet](https://image-net.org/) and made it available for download [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications)), we would like to build up a minimalistic web based \"application\" (basically a webpage in static HTML) that can accept an image upload and return the top 3 predictions in a really \"bare bones\" format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBzH2mY8E1h"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "We will use the minimalist Python webserver [Flask](https://flask.palletsprojects.com/en/2.0.x/) to build our solution.\n",
        "\n",
        "As for easy accessibility through Colab (aka. \"someone else's computer\") we use [Ngrok](https://ngrok.com/) to expose our work to be available from \"outside\" the Colab machine, that is, thorough our browser. Luckily, we don't have to deal to much with this, we just use the [Flask-Ngrok](https://github.com/gstaff/flask-ngrok) package, that handles the hassle for us. Only thing to keep in mind is, that if we run the notebook in Colab, the solution we build will __only be reachable via the ugly looking Ngrok link__, not the \"localhost\" or \"127.0.0.1\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOv_Cc9EpbVd"
      },
      "outputs": [],
      "source": [
        "!pip install flask --quiet\n",
        "!pip install flask-ngrok --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwOtg13f8E1i"
      },
      "source": [
        "Fro image formatting we will use the [PIL](https://pillow.readthedocs.io/en/stable/) Python Imaging Library and [Numpy](https://numpy.org/) is always nice to have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2zmjk5K26mT9"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tci-g2GF8E1i"
      },
      "source": [
        "## Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JJxW3wPVq8m0"
      },
      "outputs": [],
      "source": [
        "from  tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8FaziY2OrRDk",
        "outputId": "7be5c8af-ba71-4b65-a458-6f4733a96239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = VGG16()\n",
        "# Luckily, the instantiation of a pretrained model obejct\n",
        "# does a complete download in the background\n",
        "# then initializes the model with the pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDZijTRw8E1j"
      },
      "source": [
        "## Image preprocessing and prediction functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RU93x_M76JBD"
      },
      "outputs": [],
      "source": [
        "import sys, json\n",
        "\n",
        "def load_image(filename):\n",
        "    \"\"\"Given a filename, assuming it is an image, we load it from disc.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        Name of input file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PIL Image object\"\"\"\n",
        "\n",
        "\n",
        "    #TODO: use the Image class we imported from PIL to load the image.\n",
        "    #Keep it simple!\n",
        "    image = Image.open(filename) # Use Image.open to load the image.\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QskFXgSi8E1k"
      },
      "outputs": [],
      "source": [
        "def resize_image(image):\n",
        "    \"\"\"Given a PIL image object we resize it to fit into our downloaded model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : PIL Image object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PIL Image object\"\"\"\n",
        "    model_accepted_shape = model.inputs[0].shape[1:3] #ugly magic to get input shape from model\n",
        "    resized_image = image.resize(model_accepted_shape) # Resize the image using the model's input shape.\n",
        "    return resized_image\n",
        "    #TODO: please use the shape above to reshape the image with PIL and return the image object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "adbu1stl8E1k"
      },
      "outputs": [],
      "source": [
        "def to_numpy(image):\n",
        "    \"\"\"Given a PIL image object we convert it to a Numpy array.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : PIL Image object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy.Ndarray\"\"\"\n",
        "    array = np.array(image)\n",
        "    #Observe: Numpy is intelligent enough to accept a PIL object as input\n",
        "    #and convert it\n",
        "    return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mop8yYx48E1k"
      },
      "outputs": [],
      "source": [
        "def check_channels(array):\n",
        "    \"\"\"Given a numpy tensor we check for it's dimensionality,\n",
        "    and if too many channels are there, we drop 1.\n",
        "    This can be necessary in case of certain PNG images having additional channels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    array : Ndarray\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy.Ndarray\"\"\"\n",
        "    if array.shape[2]>3:\n",
        "        array = array[:,:,:3]\n",
        "    #Ugly magic to drop a channel - the last one - in cease there are too many\n",
        "    return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNUVC9hi8E1k"
      },
      "outputs": [],
      "source": [
        "def create_batch(array):\n",
        "    \"\"\"Given a numpy tensor we create a single element \"batch\" from it by adding a dimension.\n",
        "    This is necessary because the model was trained with \"minibatches\" of data,\n",
        "    so it assumes, that it does not get one single unput, but a bunch of them at once.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    array : Ndarray\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy.Ndarray\"\"\"\n",
        "    #TODO\n",
        "    #You have to extend the dimensionality of the input...\n",
        "    batch = np.expand_dims(array, axis=0) # Extend the dimensionality of the input array.\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rCD2F7V38E1k"
      },
      "outputs": [],
      "source": [
        "def execute_prediction(batch, model):\n",
        "    \"\"\"Given a numpy tensor representing the \"batch\" and the loaded model object,\n",
        "    we execute the prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : Ndarray,\n",
        "    model : TF.Keras.model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prediction in Keras model's own format\"\"\"\n",
        "    #TODO\n",
        "     #Use the appropriate method of the model on the data\n",
        "    prediction = model.predict(batch) # Use the model's predict method on the batch.\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ieeQJEts8E1l"
      },
      "outputs": [],
      "source": [
        "def interpret_prediciton(prediction):\n",
        "    \"\"\"Given Keras model's prediction, we parse it and select the top 3.\n",
        "    For this ve utilize the default `decode_predictions` function of TF.Keras\n",
        "    For more info, see https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/decode_predictions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    array : Keras model's prediction\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary containing the top 3 predictions.\"\"\"\n",
        "    label = decode_predictions(prediction)\n",
        "    decoded_prediction = []\n",
        "    # retrieve the most likely result, e.g. highest probability\n",
        "    for l in label[0][:3]:\n",
        "        print('%s (%.2f%%)' % (l[1], round(l[2]*100,2)))\n",
        "        #some nice formatting to dicts\n",
        "        decoded_prediction.append({\"class\": l[1], \"probability\":round(l[2]*100,2)})\n",
        "    return decoded_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "URYIN2TQ8E1l"
      },
      "outputs": [],
      "source": [
        "def human_format_prediction(decoded_prediction):\n",
        "    \"\"\"Given the processed prediction dict, we modify it to look nice to people\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dict : Interpreted prediction top 3\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A string representing human readable output.\"\"\"\n",
        "    formatted_output = \"\"\n",
        "    for element in decoded_prediction:\n",
        "        formatted_output += str(element)\n",
        "        formatted_output += \"<br>\" #some nice line breaks in HTML rendering\n",
        "    return formatted_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pJujECyv8E1l"
      },
      "outputs": [],
      "source": [
        "def json_format_prediction(decoded_prediction):\n",
        "    \"\"\"Given the processed prediction dict, we modify it to become proper JSON\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dict : Interpreted prediction top 3\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A JSON conformant string representing of the output.\"\"\"\n",
        "    return json.dumps(decoded_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AdX5O06t8E1l"
      },
      "outputs": [],
      "source": [
        "def do_prediction(filename):\n",
        "    \"\"\"Main prediction function.\n",
        "    - Takes in a filename,\n",
        "    - calls preprocessing steps,\n",
        "    - returns formatted prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        Name of input file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A string representingoutput.\"\"\"\n",
        "    image = load_image(filename)\n",
        "    image = resize_image(image)\n",
        "    array = to_numpy(image)\n",
        "    array = check_channels(array)\n",
        "    batch = create_batch(array)\n",
        "    prediction = execute_prediction(batch, model)\n",
        "    decoded_prediction = interpret_prediciton(prediction)\n",
        "    formatted_output = human_format_prediction(decoded_prediction)\n",
        "    return formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkzUWktw3EvB"
      },
      "source": [
        "Please observe, that the model will systemmatically err in the direction fo \"slim\" dog breeds, because the way we implemented \"resize\" of the original image - ie. we \"squeeze\" te image to conform to 224x224 pixels!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6pUZ2D38E1l"
      },
      "source": [
        "## Flask app\n",
        "\n",
        "Since we have nice functions that can take in an image and return a prediction, we can now start to develop the mini \"webapp\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBRNcgD78E1l"
      },
      "source": [
        "### Basic HTML page\n",
        "\n",
        "Let's first define a basic HTML page layout tha thas a single control element: an upload \"form\" which can take in files via the browser. This will be our way to feed in the inputs for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E8Lf85df4Fxg"
      },
      "outputs": [],
      "source": [
        "HTML = \"\"\"\n",
        "<!doctype html>\n",
        "<html>\n",
        "  <head>\n",
        "    <title>File Upload</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1>File Upload</h1>\n",
        "    <form method=\"POST\" action=\"\" enctype=\"multipart/form-data\">\n",
        "      <p><input type=\"file\" name=\"file\"></p>\n",
        "      <p><input type=\"submit\" value=\"Submit\"></p>\n",
        "    </form>\n",
        "  </body>\n",
        "</html>\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WioWTV3O8E1m"
      },
      "source": [
        "### \"Main app\"\n",
        "\n",
        "Now we define a minimalistic Flask app, that defines single page endpoint at \"/\", so the root URL.\n",
        "\n",
        "If a HTTP (\"GET\") request comes in to this URL, the app will automatically respond by \"serving\" the HTML content we difend above, so the requesting browser can render the \"webpage\" we built.\n",
        "\n",
        "After this the user is free to interact with our wonderful form to choose an appropriate image file. We are assuming here that the user acts non-maliciously, so we do not do all the checks for file content and structure that would protect us, just assume, that the user uploads well behaving image files.\n",
        "\n",
        "When the user pushes \"Submit\", this automatically generates a \"POST\" HTTP request towards our Flask app's \"/\" or \"root\" endpoint and with it, it \"posts\" or sends the image file, which we handle by defining an `upload_file` endpoint, that saves the file and calls on it the `do_prediction` function we defined above. (Please observe, that we don't care about the saved files, so they might accumuate, collide...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hJq8r9_hprWN"
      },
      "outputs": [],
      "source": [
        "from flask import *\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "#TODO Use the Flask guide to define the simplest possible endpoint for the \"root url\"\n",
        "@app.route(\"/\", methods=['GET', 'POST'])\n",
        "def index():\n",
        "    if request.method == 'POST':\n",
        "        uploaded_file = request.files['file']\n",
        "        if uploaded_file.filename != '':\n",
        "            uploaded_file.save(uploaded_file.filename)\n",
        "\n",
        "        result = do_prediction(uploaded_file.filename)  # Use the main prediction function\n",
        "        return result\n",
        "    else:\n",
        "        return HTML  # Return the HTML content for GET requests\n",
        "\n",
        "#TODO Use the Flask guide to define a POST endpoint for the \"root url\"\n",
        "@app.route(\"/\", methods=['POST'])\n",
        "def upload_file():\n",
        "    uploaded_file = request.files['file']\n",
        "    if uploaded_file.filename != '':\n",
        "        uploaded_file.save(uploaded_file.filename)\n",
        "\n",
        "    # Use the main prediction function from above to generate the result\n",
        "    result = do_prediction(uploaded_file.filename)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLzunDBC8E1m"
      },
      "source": [
        "And finally we start running our \"web application\" via the Ngrok tunnel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEnIz4_UpzED",
        "outputId": "582847ee-06b2-4fc2-a205-1e68b4922bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Exception in thread Thread-10:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 199, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 495, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 441, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 279, in connect\n",
            "    self.sock = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 214, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7cbe703a0d60>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7cbe703a0d60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1378, in run\n",
            "    self.function(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
            "    ngrok_address = _run_ngrok()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 35, in _run_ngrok\n",
            "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 700, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7cbe703a0d60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        }
      ],
      "source": [
        "run_with_ngrok(app)\n",
        "app.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiVrjEC08E1m"
      },
      "source": [
        "In case local execution fails, [this](https://githubmemory.com/repo/gstaff/flask-ngrok/issues/20) might help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2GwX_Ym8E1m"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Please open the temporary ngrok.io link that is visible in the cell output, grab a pictrure from somewhere (eg. an image of a dog) and use the submit functionality!\n",
        "\n",
        "After a single prediction, you can get back to the main form with the back button."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Flask_example.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}